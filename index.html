<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Parallelized PageRank Project</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            background-color: #f9f9f9;
        }
        header, footer {
            text-align: center;
            padding: 10px;
            background-color: #003366;
            color: white;
        }
        .tabs {
            display: flex;
            justify-content: center;
            background-color: #003366;
            padding: 0;
            margin: 0;
            border-bottom: 3px solid #002244;
        }
        .tab-button {
            background-color: #003366;
            color: white;
            border: none;
            padding: 15px 30px;
            cursor: pointer;
            font-size: 16px;
            transition: background-color 0.3s;
            border-right: 1px solid #002244;
        }
        .tab-button:hover {
            background-color: #004080;
        }
        .tab-button.active {
            background-color: #0059b3;
            font-weight: bold;
        }
        .tab-content {
            display: none;
            padding: 20px;
            animation: fadeIn 0.5s;
        }
        .tab-content.active {
            display: block;
        }
        @keyframes fadeIn {
            from { opacity: 0; }
            to { opacity: 1; }
        }
        section {
            background-color: white;
            padding: 15px 20px;
            margin: 15px 0;
            border-radius: 8px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        a {
            color: #003366;
        }
        ul {
            list-style-type: disc;
            padding-left: 20px;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin-top: 10px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #003366;
            color: white;
        }
    </style>
</head>
<body>
    <header>
        <h1>Parallelized PageRank With OpenMP and CUDA</h1>
        <p>Varshini Subramanian (vsubrama), Mahati Manda (mmanda)</p>
    </header>

    <div class="tabs">
        <button class="tab-button active" onclick="openTab(event, 'milestone')">Milestone Report</button>
        <button class="tab-button" onclick="openTab(event, 'proposal')">Project Proposal</button>
    </div>

    <div id="milestone" class="tab-content active">
        <section>            
            <h3>Initial Schedule</h3>
            <table>
                <thead>
                    <tr>
                        <th>Week</th>
                        <th>Day</th>
                        <th>Date</th>
                        <th>Weekly Goal</th>
                        <th>Deadline</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="7">1</td>
                        <td>M</td>
                        <td>11/17/2025</td>
                        <td rowspan="7">
                            <del>Implement Sequential PageRank</del><br>
                            <del>Create validation script to check other programs against</del><br>
                            <del>Implement OpenMP Parallelization</del><br>
                            Evaluate OpenMP Performance
                        </td>
                        <td rowspan="7">Project Proposal Due</td>
                    </tr>
                    <tr><td>T</td><td>11/18/2025</td></tr>
                    <tr><td>W</td><td>11/19/2025</td></tr>
                    <tr><td>Th</td><td>11/20/2025</td></tr>
                    <tr><td>F</td><td>11/21/2025</td></tr>
                    <tr><td>Sa</td><td>11/22/2025</td></tr>
                    <tr><td>Su</td><td>11/23/2025</td></tr>
                    
                    <tr>
                        <td rowspan="7">2</td>
                        <td>M</td>
                        <td>11/24/2025</td>
                        <td rowspan="7">
                            Implement MPI Parallelization<br>
                            Evaluate MPI Performance<br>
                            Implement CUDA Parallelization<br>
                            Evaluate CUDA Performance
                        </td>
                        <td rowspan="7"></td>
                    </tr>
                    <tr><td>T</td><td>11/25/2025</td></tr>
                    <tr><td>W</td><td>11/26/2025</td></tr>
                    <tr><td>Th</td><td>11/27/2025</td></tr>
                    <tr><td>F</td><td>11/28/2025</td></tr>
                    <tr><td>Sa</td><td>11/29/2025</td></tr>
                    <tr><td>Su</td><td>11/30/2025</td></tr>
                    
                    <tr>
                        <td rowspan="7">3</td>
                        <td>M</td>
                        <td>12/1/2025</td>
                        <td rowspan="7">
                            Further Performance Evaluation<br>
                            If time: Implement Hybrid Parallelism or Gauss-Seidel PageRank Algorithm<br>
                            Finalize Report
                        </td>
                        <td rowspan="7">Milestone Report Due</td>
                    </tr>
                    <tr><td>T</td><td>12/2/2025</td></tr>
                    <tr><td>W</td><td>12/3/2025</td></tr>
                    <tr><td>Th</td><td>12/4/2025</td></tr>
                    <tr><td>F</td><td>12/5/2025</td></tr>
                    <tr><td>Sa</td><td>12/6/2025</td></tr>
                    <tr><td>Su</td><td>12/7/2025</td></tr>
                    
                    <tr>
                        <td>4</td>
                        <td>M</td>
                        <td>12/8/2025</td>
                        <td></td>
                        <td>Final Report Due</td>
                    </tr>
                </tbody>
            </table>

            <h3>Revised Plan</h3>
            <table>
                <thead>
                    <tr>
                        <th>Week</th>
                        <th>Day</th>
                        <th>Date</th>
                        <th>Tasks</th>
                        <th>Deadline</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td rowspan="7">3</td>
                        <td>M</td>
                        <td>12/1/2025</td>
                        <td></td>
                        <td>Milestone Report Due</td>
                    </tr>
                    <tr>
                        <td>T</td>
                        <td>12/2/2025</td>
                        <td>Finish CUDA Implementation - Mahati + Varshini<br>Finalize more extensive test cases for evaluation - Mahati + Varshini</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>W</td>
                        <td>12/3/2025</td>
                        <td>Gather baseline performance metrics for sequential implementation - Mahati + Varshini<br>Gather preliminary performance metrics for OpenMP and tweak implementation as necessary - Varshini</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Th</td>
                        <td>12/4/2025</td>
                        <td>Gather final performance metrics for OpenMP - Varshini<br>Gather preliminary performance metrics for CUDA, tweak implementation as necessary - Mahati</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>F</td>
                        <td>12/5/2025</td>
                        <td>Gather final performance metrics for CUDA - Mahati<br>Finalize all results for final report - Mahati + Varshini</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Sa</td>
                        <td>12/6/2025</td>
                        <td>Run further scripts to include demos in final report - Mahati + Varshini<br>Work on report - Mahati + Varshini</td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>Su</td>
                        <td>12/7/2025</td>
                        <td>Finalize report - Mahati + Varshini</td>
                        <td></td>
                    </tr>
                    
                    <tr>
                        <td>4</td>
                        <td>M</td>
                        <td>12/8/2025</td>
                        <td></td>
                        <td>Final Report Due</td>
                    </tr>
                </tbody>
            </table>

            <h3>Work Completed</h3>
            <p>Thus far, we have completed both our sequential and OpenMP implementations, and verified their correctness by testing on several simple graphs (ranging from 3-10 nodes). We also finished our testing suite (primarily consisting of our validation script) that more extensive test cases can plug into to benchmark our sequential and parallel implementations. Lastly, we started and are currently working on our CUDA implementation, as well as generating graphs with a couple thousand vertices to use for benchmarking.</p>

            <h3>Goals and Deliverables</h3>
            <p>Based on the feedback we received on our proposal, we decided to focus more on analyzing parallelism bottlenecks due to the memory bound nature of our problems, as well as optimizing parallel operations to be more efficient based on this constraint. Therefore, we decided to scrap our MPI implementation, and instead focus on parallelism with just OpenMP and CUDA.</p>
            
            <p>Additionally, due to unexpected travel conflicts, we are slightly behind on finishing our CUDA implementation and evaluation; however, our original plan did account for this, and included this week as a buffer week, so we will finish our intended goals with the revised plans. Therefore, we believe we can achieve our goals of a sequential baseline implementation, 2 parallel implementations (using OpenMP and CUDA), and performance evaluation/speedup analysis. However, we likely will not have time to implement our nice-to-have goals of hybrid parallelism and Gauss-Seidel PageRank. The updated list of goals is as follows:</p>
            
            <ul>
                <li><strong>Sequential Baseline Implementation:</strong> This will provide us a baseline for speedup calculations and a reference for correctness</li>
                <li><strong>Parallel Implementations with OpenMP and CUDA:</strong> We will perform power-iteration PageRank until convergence using a CSR/adjacency-list graph representation for each implementation.</li>
                <li><strong>Performance Evaluation and Speedup Analysis:</strong> We will measure the strength of scaling as we increase the number of processors (OpenMP), as well as GPU throughput and utilization (CUDA).</li>
            </ul>

            <h3>Issues</h3>
            <p>No major issues - we just need to finish our implementations and get to benchmarking!</p>
        </section>
    </div>

    <div id="proposal" class="tab-content">
        <section>  
            <h3>Summary</h3>
            <p>We plan to implement and optimize a parallelized version of the PageRank algorithm using three complementary parallel programming models: OpenMP, MPI, and CUDA, to measure the importance of different nodes in a network. Our goal is to design and benchmark our parallel implementation and compare it against the sequential implementation and potentially other parallel implementations if time persists. </p>

            <h3>Background</h3>
            <p>We are interested in parallelizing the PageRank algorithm, which was developed by Google founders for web search ranking based on a numerical importance metric. PageRank works by treating the web as a graph, where webpages are nodes and hyperlinks are edges – a particular webpage receives a high importance score if many pages link to it, or if it is linked by highly ranked pages, thus factoring in both quantity and quality of links. The pseudocode for the algorithm is as follows: </p>
            <img src="10649542-fig-2-source-large.gif" alt="PageRank Pseudocode" height=500>
            <p>While primarily used for webpage ranking, the PageRank algorithm can also be used for other similar purposes, such as recommendation systems and ranking papers in journals.</p>
            <p>There are several key components of PageRank that allow it to benefit from parallelism. Within each iteration, the core operation is evaluating the rank update for each node by aggregating contributions from incoming neighbors – in a sequential implementation, this would be done one node at a time, but a parallel implementation can compute different nodes' updates in parallel, which would improve performance. Additionally, both sparse vector-matrix multiplication (used for traversing adjacency lists) and the iterative power method are highly parallelizable algorithms, so we hope to exploit parallelism here as well.</p>

            <h3>Challenge</h3>
            <p>While PageRank lends itself naturally to parallelism, this project is challenging due to performance challenges that will arise from the structure of large real-world graphs and the memory-bound nature of the algorithm. The algorithm requires scanning numerous edges and each iteration has little computation per edge, but requires significant memory movement. This makes overall performance sensitive to memory bandwidth, memory access patterns, and communication overhead in distributed settings. We hope to learn how different parallel paradigms handle irregular, sparse, memory-bound workloads, what bottlenecks will dominate in each model, how graph structure affects load balancing and scaling, and how effectively parallelism can accelerate PageRank given its constraints. </p>
            <p>PageRank uses an iterative power method, so within each iteration, updates to nodes are independent as long as the previous iteration's vector is read-only. Because of this, we plan to parallelize node computations, as well as the iterative power method/sparse matrix-vector multiplication, using the different programming models.

                However, between iterations, each step depends on the results of the previous step. This is where our dependency would be and where we would need to work to synchronize the program to ensure the correctness of our implementation. 
            </p>
            <p>
                PageRank is primarily memory-bound rather than compute-bound so some constraints would be related to how much memory is required to store large graphs, particularly when dealing with dense matrices. Additionally, cache locality would become a significant constraint as PageRank's random access patterns and large vector sizes mean that vector value accesses may exceed cache capacity often, leading to costly memory transfers. Thus, performance is restricted by memory bandwidth, cache efficiency, and whether we can fit the critical working set in fast-access memory. 
            </p>

            <h3>Resources</h3>
            <p>
                We will be using multi-core CPU nodes and MPI capable clusters on the PSC Bridges-2 machines if provided which will allow us to run both OpenMP and MPI experiments at varying thread and processor counts. Additionally, to implement parallelism using CUDA, we will also need to use GPU nodes so we can evaluate the performance of PageRank under parallel execution. However, similar to the previous assignments, we will use the GHC machines for debugging purposes and reserve PSC use only for evaluation. We will be starting from the sequential PageRank code and then trying to parallelize that. The paper we will use as reference to how to do this is: "Performance Analysis of Parallelized PageRank Algorithm using OpenMP, MPI and CUDA" where we are given a starting point on how to implement the PageRank Algorithm in a parallel fashion. For now we think these resources would be sufficient and would benefit from access to the PSC machines. 
            </p>

            <h3>Goals / Deliverables</h3>
            <h4>Plan to Achieve</h4>
            <ul>
                <li>Sequential Baseline Implementation: This will provide us a baseline for speedup calculations and a reference for correctness            

                </li>
                <li>3 Separate Parallel Implementations with OpenMP, MPI, CUDA: We will perform power-iteration PageRank until convergence using a CSR/adjacency-list graph representation for each implementation. If we are unable to do all 3 we will at least plan to create implementations of 2 of these methods            

                </li>
                <li>
                    Performance Evaluation and Speedup Analysis: We will measure the strength of scaling as we increase the number of processors, GPU throughput and utilization for CUDA, computation time, total time, memory bandwidth utilization, and communication overhead for MPI. We expect that all three algorithms will satisfy correctness. We expect to achieve at least a 4x speedup with OpenMP on 16 cores, Distributed speedup with MPI, and at least 8x speedup with CUDA on a moderate sized graph. These metrics were consistent with the results achieved from previous research so we will aim for our results to be comparable or better than them.
                </li>
            </ul>

            <h4>Hope to Achieve</h4>
            <ul>
                <li>Hybrid Parallelism: we will attempt to create a hybrid parallel algorithm where we combine either MPI and OpenMP or MPI and CUDA. 
                </li>
                <li>We might also implement the asynchronous Gauss-Seidel PageRank algorithm and see if we can get better results with MPI, OpenMP, and CUDA implementations. We hypothesize that if we are able to do this we would get better speedup and memory utilization results as this algorithm would remove global iteration barriers. 
                </li>
            </ul>

            <h3>Platform Choice</h3>
            <p>
                To parallelize our algorithm, we plan to use OpenMP, MPI, and CUDA. OpenMP is well suited for PageRank, since each iteration of the PageRank algorithm contains loops over nodes in the graph that can be parallelized, which would allow us to effectively use multicore CPUs. CUDA is also a good fit, since it's naturally suited to handle the parallelization of sparse matrix-vector multiplication and iterative powers, which comprise the bulk of PageRank's computation. Lastly, we chose to explore MPI due to the memory-bound nature of PageRank - MPI allows us to partition the graph across multiple nodes, which in theory would benefit the scalability of PageRank, so we wanted to explore this as well.
            </p>

            <h3>Schedule</h3>
            <table>
                <thead>
                    <tr>
                        <th>Week</th>
                        <th>Date</th>
                        <th>Weekly Goal</th>
                        <th>Deadline</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>1</td>
                        <td>11/17–11/23</td>
                        <td>
                            Implement Sequential PageRank<br>
                            Create validation script<br>
                            Implement OpenMP parallelization<br>
                            Evaluate OpenMP performance
                        </td>
                        <td>Project Proposal Due</td>
                    </tr>
                    <tr>
                        <td>2</td>
                        <td>11/24–11/30</td>
                        <td>
                            Implement MPI parallelization<br>
                            Evaluate MPI performance<br>
                            Implement CUDA parallelization<br>
                            Evaluate CUDA performance
                        </td>
                        <td></td>
                    </tr>
                    <tr>
                        <td>3</td>
                        <td>12/1–12/7</td>
                        <td>
                            Further performance evaluation<br>
                            If time: implement hybrid or Gauss-Seidel PageRank<br>
                            Finalize report
                        </td>
                        <td>Milestone Report Due</td>
                    </tr>
                    <tr>
                        <td>4</td>
                        <td>12/8–12/12</td>
                        <td>Finalize project and submit final report</td>
                        <td>Final Report Due</td>
                    </tr>
                </tbody>
            </table>

            <h3>References</h3>
            <ul>
                <li><a href="https://en.wikipedia.org/wiki/PageRank" target="_blank">Wikipedia: PageRank</a></li>
                <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10649542" target="_blank">IEEE Paper: Parallelized PageRank</a></li>
                <li><a href="http://infolab.stanford.edu/~backrub/google.html" target="_blank">Stanford: Original PageRank Description</a></li>
            </ul>
        </section>
    </div>

    <footer>
        <p>&copy; 2025 Varshini Subramanian & Mahati Manda</p>
    </footer>

    <script>
        function openTab(evt, tabName) {
            var i, tabContent, tabButtons;
            
            // Hide all tab content
            tabContent = document.getElementsByClassName("tab-content");
            for (i = 0; i < tabContent.length; i++) {
                tabContent[i].classList.remove("active");
            }
            
            // Remove active class from all buttons
            tabButtons = document.getElementsByClassName("tab-button");
            for (i = 0; i < tabButtons.length; i++) {
                tabButtons[i].classList.remove("active");
            }
            
            // Show the current tab and mark button as active
            document.getElementById(tabName).classList.add("active");
            evt.currentTarget.classList.add("active");
        }
    </script>
</body>
</html>